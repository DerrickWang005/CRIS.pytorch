_BASE_: ./base.yaml

OUTPUT_DIR: ./output/refcoco-unc_convnext-large_step-100k_bs64_512x512pip_repeat5_pix-cross_both-train

INPUT:
  DATASET_NAME: ReferDataset
  TRAIN_ROOT: /vepfs/home/wangzhaoqing/develop/CRIS.pytorch/data/refcoco
  TRAIN_NAME: refcoco
  REFER_SPLIT: unc
  DATA_SPLIT: train
  DATA_TEST_SPLIT: val
  CROP_SIZE: 512
  POS_REPEAT: 5
  IGNORE_LABEL: 0
  FORMAT: RGB

MODEL:
  BACKBONE:
    NAME: CLIP
    CLIP_MODEL_NAME: convnext_large_d_320
    CLIP_PRETRAINED_WEIGHTS: /vepfs/home/wangzhaoqing/develop/CRIS.pytorch/pretrain_model/models--laion--CLIP-convnext_large_d_320.laion2B-s29B-b131K-ft-soup/snapshots/39918dfbdf69ccd2172e6510a430e92337ee23e1/open_clip_pytorch_model.bin
  CRIS:
    PIXEL_DECODER_NAME: MSDeformAttnPixelDecoder2
    CLS_DIM: 768

SOLVER:
  IMS_PER_BATCH: 64
  MAX_ITER: 100000
  STEPS:
  - 80000
  - 95000
  BACKBONE_MULTIPLIER: 0.01
TEST:
  EVAL_PERIOD: 10000

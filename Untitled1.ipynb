{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "425ae604",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e6032993",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'utils.zero_shot_utils'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 8\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mfairseq\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m distributed_utils, options, tasks, utils\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mfairseq\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdataclass\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m convert_namespace_to_omegaconf\n\u001b[0;32m----> 8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mzero_shot_utils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m zero_shot_step\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtasks\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmm_tasks\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mvqa_gen\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m VqaGenTask\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmodels\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mofa\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m OFAModel\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'utils.zero_shot_utils'"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import re\n",
    "from fairseq import utils,tasks\n",
    "from fairseq import checkpoint_utils\n",
    "from fairseq import distributed_utils, options, tasks, utils\n",
    "from fairseq.dataclass.utils import convert_namespace_to_omegaconf\n",
    "from utils.zero_shot_utils import zero_shot_step\n",
    "from tasks.mm_tasks.vqa_gen import VqaGenTask\n",
    "from models.ofa import OFAModel\n",
    "from PIL import Image\n",
    "\n",
    "# Register VQA task\n",
    "tasks.register_task('vqa_gen',VqaGenTask)\n",
    "\n",
    "# turn on cuda if GPU is available\n",
    "use_cuda = torch.cuda.is_available()\n",
    "# use fp16 only when GPU is available\n",
    "use_fp16 = False\n",
    "\n",
    "# specify some options for evaluation\n",
    "parser = options.get_generation_parser()\n",
    "input_args = [\"\", \"--task=vqa_gen\", \"--beam=100\", \"--unnormalized\", \"--path=checkpoints/ofa_large_384.pt\", \"--bpe-dir=utils/BPE\"]\n",
    "args = options.parse_args_and_arch(parser, input_args)\n",
    "cfg = convert_namespace_to_omegaconf(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e36a0c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load pretrained ckpt & config\n",
    "task = tasks.setup_task(cfg.task)\n",
    "models, cfg = checkpoint_utils.load_model_ensemble(\n",
    "    utils.split_paths(cfg.common_eval.path),\n",
    "    task=task\n",
    ")\n",
    "\n",
    "# Move models to GPU\n",
    "for model in models:\n",
    "    model.eval()\n",
    "    if use_fp16:\n",
    "        model.half()\n",
    "    if use_cuda and not cfg.distributed_training.pipeline_model_parallel:\n",
    "        model.cuda()\n",
    "    model.prepare_for_inference_(cfg)\n",
    "\n",
    "# Initialize generator\n",
    "generator = task.build_generator(models, cfg.generation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "689a05f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Image transform\n",
    "from torchvision import transforms\n",
    "mean = [0.5, 0.5, 0.5]\n",
    "std = [0.5, 0.5, 0.5]\n",
    "\n",
    "patch_resize_transform = transforms.Compose([\n",
    "    lambda image: image.convert(\"RGB\"),\n",
    "    transforms.Resize((cfg.task.patch_image_size, cfg.task.patch_image_size), interpolation=Image.BICUBIC),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=mean, std=std),\n",
    "])\n",
    "\n",
    "# Text preprocess\n",
    "bos_item = torch.LongTensor([task.src_dict.bos()])\n",
    "eos_item = torch.LongTensor([task.src_dict.eos()])\n",
    "pad_idx = task.src_dict.pad()\n",
    "\n",
    "# Normalize the question\n",
    "def pre_question(question, max_ques_words):\n",
    "    question = question.lower().lstrip(\",.!?*#:;~\").replace('-', ' ').replace('/', ' ')\n",
    "    question = re.sub(\n",
    "        r\"\\s{2,}\",\n",
    "        ' ',\n",
    "        question,\n",
    "    )\n",
    "    question = question.rstrip('\\n')\n",
    "    question = question.strip(' ')\n",
    "    # truncate question\n",
    "    question_words = question.split(' ')\n",
    "    if len(question_words) > max_ques_words:\n",
    "        question = ' '.join(question_words[:max_ques_words])\n",
    "    return question\n",
    "\n",
    "def encode_text(text, length=None, append_bos=False, append_eos=False):\n",
    "    s = task.tgt_dict.encode_line(\n",
    "        line=task.bpe.encode(text),\n",
    "        add_if_not_exist=False,\n",
    "        append_eos=False\n",
    "    ).long()\n",
    "    if length is not None:\n",
    "        s = s[:length]\n",
    "    if append_bos:\n",
    "        s = torch.cat([bos_item, s])\n",
    "    if append_eos:\n",
    "        s = torch.cat([s, eos_item])\n",
    "    return s\n",
    "\n",
    "# Construct input for open-domain VQA task\n",
    "def construct_sample(image: Image, question: str):\n",
    "    patch_image = patch_resize_transform(image).unsqueeze(0)\n",
    "    patch_mask = torch.tensor([True])\n",
    "\n",
    "    question = pre_question(question, task.cfg.max_src_length)\n",
    "    question = question + '?' if not question.endswith('?') else question\n",
    "    src_text = encode_text(' {}'.format(question), append_bos=True, append_eos=True).unsqueeze(0)\n",
    "\n",
    "    src_length = torch.LongTensor([s.ne(pad_idx).long().sum() for s in src_text])\n",
    "    ref_dict = np.array([{'yes': 1.0}]) # just placeholder\n",
    "    sample = {\n",
    "        \"id\":np.array(['42']),\n",
    "        \"net_input\": {\n",
    "            \"src_tokens\": src_text,\n",
    "            \"src_lengths\": src_length,\n",
    "            \"patch_images\": patch_image,\n",
    "            \"patch_masks\": patch_mask,\n",
    "        },\n",
    "        \"ref_dict\": ref_dict,\n",
    "    }\n",
    "    return sample\n",
    "  \n",
    "# Function to turn FP32 to FP16\n",
    "def apply_half(t):\n",
    "    if t.dtype is torch.float32:\n",
    "        return t.to(dtype=torch.half)\n",
    "    return t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37242d8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "image = \n",
    "question = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "290a9229",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = construct_sample(image, question)\n",
    "sample = utils.move_to_cuda(sample) if use_cuda else sample\n",
    "sample = utils.apply_to_sample(apply_half, sample) if use_fp16 else sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bea0aad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run eval step for open-domain VQA\n",
    "with torch.no_grad():\n",
    "    result, scores = zero_shot_step(task, generator, models, sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df0b699f",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(image)\n",
    "print('Question: {}'.format(question))\n",
    "print('OFA\\'s Answer: {}'.format(result[0]['answer']))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
